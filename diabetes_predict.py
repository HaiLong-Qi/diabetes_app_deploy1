import streamlit as st
import pandas as pd
import xgboost as xgb
from pathlib import Path
import joblib
import os

# é¡µé¢è®¾ç½®
st.set_page_config(page_title="Diabetes Risk Prediction", page_icon="ğŸ¥", layout="centered")

# å¯¼å…¥å¿…è¦çš„åº“ï¼ˆç§»åˆ°Streamlitåˆå§‹åŒ–ä¹‹åï¼‰
try:
    import shap
    import matplotlib.pyplot as plt
    import numpy as np
    from io import BytesIO
    import base64
    import seaborn as sns
    from matplotlib.colors import LinearSegmentedColormap
    SHAP_AVAILABLE = True
    
    # è®¾ç½®ä¸“ä¸šå­¦æœ¯å›¾è¡¨æ ·å¼
    plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['figure.dpi'] = 300
    plt.rcParams['savefig.dpi'] = 300
    
except ImportError as e:
    SHAP_AVAILABLE = False
    st.error(f"ç¼ºå°‘ä¾èµ–åº“: {e}")

# ==================== å¤šè¯­è¨€æ”¯æŒ ====================
class Translation:
    def __init__(self):
        self.chinese = {
            "title": "ğŸ¥ ç³–å°¿ç—…é£é™©é¢„æµ‹ç³»ç»Ÿ",
            "subtitle": "åŸºäºXGBoostä¸SHAPè§£é‡Šçš„æ™ºèƒ½é¢„æµ‹",
            "patient_info": "è¯·è¾“å…¥æ‚£è€…ä¸´åºŠæŒ‡æ ‡",
            "all_indicators": "è¯·å¡«å†™æ‰€æœ‰19ä¸ªä¸´åºŠæŒ‡æ ‡ï¼š",
            "predict_button": "é¢„æµ‹ç³–å°¿ç—…é£é™©",
            "result_title": "ğŸ“Š é¢„æµ‹ç»“æœ",
            "probability": "ç³–å°¿ç—…æ¦‚ç‡",
            "risk_level": "é£é™©ç­‰çº§",
            "samples": "è¯„ä¼°æ ·æœ¬",
            "medical_advice": "åŒ»å­¦å»ºè®®",
            "shap_analysis": "ğŸ“ˆ SHAPç‰¹å¾é‡è¦æ€§åˆ†æ",
            "high_risk": "ğŸ”´ é«˜é£é™©",
            "medium_risk": "ğŸŸ¡ ä¸­ç­‰é£é™©", 
            "low_risk": "ğŸŸ¢ ä½é£é™©",
            "high_risk_suggestion": "å»ºè®®ç«‹å³å°±åŒ»å¹¶è¿›è¡Œè¯¦ç»†æ£€æŸ¥ï¼ŒåŒ…æ‹¬ç³–åŒ–è¡€çº¢è›‹ç™½ã€å£æœè‘¡è„ç³–è€é‡è¯•éªŒç­‰",
            "medium_risk_suggestion": "å»ºè®®å®šæœŸç›‘æµ‹è¡€ç³–ï¼Œæ”¹å–„é¥®é£Ÿå’Œè¿åŠ¨ä¹ æƒ¯ï¼Œ3-6ä¸ªæœˆåå¤æŸ¥",
            "low_risk_suggestion": "ä¿æŒè‰¯å¥½çš„ç”Ÿæ´»ä¹ æƒ¯ï¼Œæ¯å¹´è¿›è¡Œå¸¸è§„ä½“æ£€",
            "shap_loading": "æ­£åœ¨ç”Ÿæˆä¸“ä¸šSHAPå¯è§†åŒ–...",
            "language": "è¯­è¨€",
            "urine_guide": "å°¿å¸¸è§„åˆ†çº§æŒ‡å—",
            "disclaimer_title": "å…è´£å£°æ˜",
            "disclaimer_content": """
**é‡è¦æç¤º - è¯·ä»”ç»†é˜…è¯»**

**1. æ¨¡å‹æ€§è´¨ä¸å±€é™æ€§å£°æ˜**

**éè¯Šæ–­å·¥å…·ï¼Œä»…ä¾›è¾…åŠ©å‚è€ƒ**ï¼šæ˜ç¡®å£°æ˜æœ¬æ¨¡å‹åŠå…¶é¢„æµ‹ç»“æœä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„æ­£å¼è¯Šæ–­ã€‚å®ƒä»…ä½œä¸ºä¸€ä¸ªè¾…åŠ©æ€§çš„é£é™©è¯„ä¼°å’Œå†³ç­–æ”¯æŒå·¥å…·ã€‚

**åŸºäºæ¦‚ç‡ä¸ç»Ÿè®¡**ï¼šè¯´æ˜æ¨¡å‹çš„é¢„æµ‹ç»“æœæ˜¯åŸºäºç¾¤ä½“æ•°æ®å’Œç»Ÿè®¡æ¦‚ç‡å¾—å‡ºçš„ï¼Œä¸å…·æœ‰ç¡®å®šæ€§ã€‚å®ƒè¯„ä¼°çš„æ˜¯é£é™©é«˜ä½ï¼Œè€Œéç»™å‡ºæ˜¯æˆ–å¦çš„ç»å¯¹ç»“è®ºã€‚

**å­˜åœ¨ä¸ç¡®å®šæ€§**ï¼šæ˜ç¡®æŒ‡å‡ºæ‰€æœ‰é¢„æµ‹éƒ½å­˜åœ¨ä¸€å®šç¨‹åº¦çš„é”™è¯¯ç‡ï¼ŒåŒ…æ‹¬å‡é˜³æ€§ï¼ˆé¢„æµ‹æœ‰ç—…ï¼Œå®é™…æ— ç—…ï¼‰å’Œå‡é˜´æ€§ï¼ˆé¢„æµ‹æ— ç—…ï¼Œå®é™…æœ‰ç—…ï¼‰ã€‚

**2. é€‚ç”¨èŒƒå›´ä¸æ•°æ®åŸºç¡€å£°æ˜**

**è®­ç»ƒæ•°æ®æ¥æº**ï¼šæ¨¡å‹åŸºäº26,294ä¸ªåŒ»ç–—æ ·æœ¬æ•°æ®è¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«å°¿å¸¸è§„å’Œè¡€å¸¸è§„æŒ‡æ ‡ã€‚æ¨¡å‹åœ¨ä¸åŒäººç¾¤ä¸­çš„é€‚ç”¨æ€§å¯èƒ½æœ‰é™ã€‚

**é€‚ç”¨ä¸ä¸é€‚ç”¨åœºæ™¯**ï¼š
- **é€‚ç”¨**ï¼šç”¨äºé«˜å±äººç¾¤çš„åˆæ­¥ç­›æŸ¥ã€è¾…åŠ©åŒ»ç”Ÿè¿›è¡Œé‰´åˆ«è¯Šæ–­
- **ä¸é€‚ç”¨**ï¼šä¸é€‚ç”¨äºæ€¥è¯Šç”Ÿå‘½å†³ç­–ã€ä¸é€‚ç”¨äºå­•å¦‡æˆ–ç‰¹å®šç½•è§ç—…æ‚£è€…

**3. ç”¨æˆ·/åŒ»ç”Ÿè´£ä»»ä¸ä¹‰åŠ¡**

**å¿…é¡»ç»“åˆä¸´åºŠåˆ¤æ–­**ï¼šå¼ºè°ƒåŒ»ç”Ÿå¿…é¡»å°†æ¨¡å‹é¢„æµ‹ç»“æœä¸æ‚£è€…çš„å®Œæ•´ä¸´åºŠä¿¡æ¯ç›¸ç»“åˆã€‚

**æœ€ç»ˆå†³ç­–è´£ä»»æ–¹**ï¼šæ˜ç¡®å£°æ˜æœ€ç»ˆçš„è¯Šæ–­å’Œæ²»ç–—æ–¹æ¡ˆè´£ä»»å®Œå…¨åœ¨äºä¸»æ²»åŒ»ç”Ÿå’Œæ‚£è€…æœ¬äººã€‚

**ç¦æ­¢æ‚£è€…è‡ªè¡Œè§£è¯»ä¸å†³ç­–**ï¼šå¼ºçƒˆå»ºè®®æ‚£è€…ä¸è¦ä»…æ ¹æ®æ¨¡å‹é¢„æµ‹ç»“æœè¿›è¡Œè‡ªæˆ‘è¯Šæ–­ã€‚

**4. å¼€å‘è€…/æä¾›æ–¹è´£ä»»é™åˆ¶**

**æŒ‰åŸæ ·æä¾›**ï¼šå£°æ˜æ¨¡å‹æ˜¯æŒ‰åŸæ ·å’Œç°æœ‰æä¾›çš„ï¼Œä¸æ‰¿è¯ºå…¶å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€å¯é æ€§ã€‚

**ä¸æ‰¿æ‹…åŒ»ç–—è´£ä»»**ï¼šæ˜ç¡®å…é™¤å› ä½¿ç”¨æ¨¡å‹é¢„æµ‹ç»“æœè€Œå¯¼è‡´çš„ä»»ä½•ç›´æ¥æˆ–é—´æ¥è´£ä»»ã€‚

**5. æ•°æ®éšç§ä¸å®‰å…¨**

**ç¬¦åˆæ³•è§„**ï¼šç”¨æˆ·æ•°æ®çš„å¤„ç†å°†ä¸¥æ ¼éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„ã€‚

**åŒ¿ååŒ–ä¸è„±æ•**ï¼šæ‰¿è¯ºé‡‡å–æŠ€æœ¯æªæ–½ä¿æŠ¤æ‚£è€…éšç§ã€‚

**6. çŸ¥è¯†äº§æƒ**

å£°æ˜æ¨¡å‹ç›¸å…³çš„çŸ¥è¯†äº§æƒå½’å¼€å‘è€…æ‰€æœ‰ã€‚
"""
        }
        
        self.english = {
            "title": "ğŸ¥ Diabetes Risk Prediction System",
            "subtitle": "Intelligent Prediction with XGBoost and SHAP Explanation",
            "patient_info": "Patient Clinical Indicators",
            "all_indicators": "Please enter all 19 clinical indicators:",
            "predict_button": "Predict Diabetes Risk",
            "result_title": "ğŸ“Š Prediction Results",
            "probability": "Diabetes Probability",
            "risk_level": "Risk Level",
            "samples": "Evaluated Samples", 
            "medical_advice": "Medical Advice",
            "shap_analysis": "ğŸ“ˆ SHAP Feature Importance Analysis",
            "high_risk": "ğŸ”´ High Risk",
            "medium_risk": "ğŸŸ¡ Medium Risk",
            "low_risk": "ğŸŸ¢ Low Risk",
            "high_risk_suggestion": "Recommend immediate medical consultation and detailed examination including HbA1c, OGTT, etc.",
            "medium_risk_suggestion": "Recommend regular blood glucose monitoring, improve diet and exercise habits, recheck in 3-6 months",
            "low_risk_suggestion": "Maintain healthy lifestyle habits, undergo routine annual physical examination",
            "shap_loading": "Generating professional SHAP visualization...",
            "language": "Language",
            "urine_guide": "Urinalysis Grading Guide",
            "disclaimer_title": "Disclaimer",
            "disclaimer_content": """
**Important Notice - Please Read Carefully**

**1. Nature and Limitations of the Model**

**Not a Diagnostic Tool, For Reference Only**: This model and its predictions are not a substitute for formal diagnosis by qualified healthcare professionals.

**Based on Probability and Statistics**: Predictions are derived from population data and statistical probabilities, not deterministic.

**Inherent Uncertainty**: All predictions carry error rates including False Positives and False Negatives.

**2. Scope of Application and Data Foundation**

**Training Data Source**: Model was trained on 26,294 medical samples. Applicability may be limited in different populations.

**Intended and Non-Intended Use Cases**:
- **Intended Use**: For preliminary screening, to assist physicians in differential diagnosis
- **Non-Intended Use**: Not for emergency decision making, not for pregnant women

**3. User/Physician Responsibilities and Obligations**

**Must Be Integrated with Clinical Judgment**: Healthcare professionals must integrate predictions with complete clinical information.

**Ultimate Decision-Making Responsibility**: Final diagnosis and treatment decisions rest solely with treating physician.

**Prohibition of Self-Interpretation**: Patients should not use predictions for self-diagnosis.

**4. Liability Limitations**

**As Is Provision**: Model provided as is with no warranties of accuracy or reliability.

**No Medical Liability**: Provider disclaims liability for consequences from model use.

**5. Data Privacy and Security**

**Regulatory Compliance**: Data handling strictly adheres to relevant laws.

**Anonymization**: Technical measures protect patient privacy.

**6. Intellectual Property**

All intellectual property rights belong to the developer.
"""
        }

# åˆå§‹åŒ–ç¿»è¯‘
trans = Translation()

# è¯­è¨€åˆ‡æ¢
if 'language' not in st.session_state:
    st.session_state.language = 'chinese'

def get_text(key):
    return trans.chinese[key] if st.session_state.language == 'chinese' else trans.english[key]

# è¯­è¨€åˆ‡æ¢æŒ‰é’®
col_lang1, col_lang2, col_lang3 = st.columns([1, 2, 1])
with col_lang2:
    lang_option = st.radio(
        get_text("language"),
        ["ä¸­æ–‡", "English"],
        horizontal=True,
        index=0 if st.session_state.language == 'chinese' else 1
    )
    
    # æ›´æ–°è¯­è¨€çŠ¶æ€
    if lang_option == "ä¸­æ–‡":
        st.session_state.language = 'chinese'
    else:
        st.session_state.language = 'english'

# æ ‡é¢˜
st.title(get_text("title"))
st.markdown(f"**{get_text('subtitle')}**")
st.markdown("---")

# ==================== æ¨¡å‹åŠ è½½ ====================
@st.cache_resource
def load_model_and_explainer():
    try:
        st.info("æ­£åœ¨åŠ è½½æ•°æ®å¹¶è®­ç»ƒæ¨¡å‹..." if st.session_state.language == 'chinese' else "Loading data and training model...")
        data_dir = Path(r'C:\Users\13003\Desktop\Fusion_XGBoost_SHAP_Output')
        train_features = pd.read_csv(data_dir / 'fusion_train_features.csv')
        train_labels = pd.read_csv(data_dir / 'fusion_train_labels.csv')['DiabetesLabel']
        
        # è®­ç»ƒXGBoostæ¨¡å‹
        model = xgb.XGBClassifier(
            random_state=42, 
            eval_metric='logloss',
            n_estimators=100,
            max_depth=6
        )
        
        with st.spinner('æ­£åœ¨è®­ç»ƒæ¨¡å‹...' if st.session_state.language == 'chinese' else 'Training model...'):
            model.fit(train_features, train_labels)
        
        # åˆ›å»ºSHAPè§£é‡Šå™¨
        explainer = None
        if SHAP_AVAILABLE:
            with st.spinner('æ­£åœ¨åˆ›å»ºSHAPè§£é‡Šå™¨...' if st.session_state.language == 'chinese' else 'Creating SHAP explainer...'):
                explainer = shap.TreeExplainer(model)
        
        success_msg = "æ¨¡å‹è®­ç»ƒå®Œæˆï¼" if st.session_state.language == 'chinese' else "Model training completed!"
        if explainer:
            success_msg += " SHAPè§£é‡Šå™¨å°±ç»ªï¼" if st.session_state.language == 'chinese' else " SHAP explainer ready!"
        st.success(success_msg)
        return model, explainer, train_features.columns.tolist()
        
    except Exception as e:
        st.error(f"é”™è¯¯: {str(e)}" if st.session_state.language == 'chinese' else f"Error: {str(e)}")
        return None, None, None

model, explainer, feature_names = load_model_and_explainer()

if model is None:
    st.stop()

# ==================== ç®€æ´ç‰ˆSHAPå¯è§†åŒ–ï¼ˆæ— å°åœ†ç‚¹ç‰ˆæœ¬ï¼‰ ====================
def create_clean_shap_plot(input_data, prediction_prob):
    """åˆ›å»ºç®€æ´ç‰ˆSHAPå¯è§†åŒ–ï¼ˆæ— å°åœ†ç‚¹ï¼‰"""
    if not SHAP_AVAILABLE or explainer is None:
        return None
        
    try:
        with st.spinner(get_text("shap_loading")):
            # è®¡ç®—SHAPå€¼
            shap_values = explainer.shap_values(input_data)
            
            # å®šä¹‰é¢œè‰²æ–¹æ¡ˆ
            category_colors = {
                'urine': '#2E86AB',
                'blood': '#A23B72', 
                'common': '#4CAF50'
            }

            def categorize_feature(feature_name):
                if 'Urine' in feature_name:
                    return 'urine'
                elif 'Blood' in feature_name:
                    return 'blood'
                elif feature_name in ['Urine_Gender', 'Urine_Age']:
                    return 'common'
                else:
                    return 'blood'

            # è®¡ç®—å…¨å±€ç‰¹å¾é‡è¦æ€§
            shap_values_array = shap_values.values if hasattr(shap_values, 'values') else shap_values
            global_importance = np.mean(np.abs(shap_values_array), axis=0)

            feature_importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Global_Importance': global_importance,
                'Category': [categorize_feature(feat) for feat in feature_names]
            }).sort_values('Global_Importance', ascending=True)

            positive_class_contrib = np.mean(np.maximum(shap_values_array, 0), axis=0)
            negative_class_contrib = np.mean(np.maximum(-shap_values_array, 0), axis=0)

            # åˆ›å»ºå›¾è¡¨ - å¢åŠ é«˜åº¦é¿å…é¡¶éƒ¨è¢«é®æŒ¡
            fig, (ax_main, ax_legend) = plt.subplots(1, 2, figsize=(20, 14), 
                                                    gridspec_kw={'width_ratios': [3, 1]})

            sorted_features = feature_importance_df['Feature'].tolist()
            y_pos = np.arange(len(sorted_features))

            # ä¸»å›¾åŒºåŸŸ
            ax_top = ax_main.twiny()

            max_importance = np.max(global_importance)
            importance_ratios = global_importance / max_importance
            max_bar_value = np.max(global_importance) * 1.2

            # ç»˜åˆ¶å †å æ¡å½¢å›¾ï¼ˆæ— å°åœ†ç‚¹ï¼‰
            for i, feature in enumerate(sorted_features):
                category = categorize_feature(feature)
                color = category_colors[category]
                
                feature_idx = feature_names.index(feature)
                total_importance = global_importance[feature_idx]
                pos_contrib = positive_class_contrib[feature_idx]
                neg_contrib = negative_class_contrib[feature_idx]
                
                ax_top.barh(i, total_importance, color=color, alpha=0.3, height=0.8, 
                           edgecolor=color, linewidth=0.5, left=0)
                ax_top.barh(i, pos_contrib, left=0, color=color, alpha=0.8, height=0.6)
                ax_top.barh(i, neg_contrib, left=pos_contrib, color=color, alpha=0.5, height=0.6)

            ax_top.set_xlim(0, max_bar_value)
            ax_top.set_xlabel('Global Feature Importance\n(Mean |SHAP Value|)', 
                            fontsize=11, fontweight='bold', labelpad=10)
            ax_top.spines['top'].set_visible(True)
            ax_top.tick_params(axis='x', which='major', labelsize=9)
            ax_top.grid(axis='x', alpha=0.3, linestyle='--')

            # èœ‚çªå›¾éƒ¨åˆ†
            sorted_indices = [feature_names.index(feat) for feat in sorted_features]
            sorted_shap_values = shap_values_array[:, sorted_indices]
            
            shap_abs_max = np.max(np.abs(sorted_shap_values)) * 1.1
            ax_main.set_xlim(-shap_abs_max, shap_abs_max)

            scatter_plot = None
            for i, feature in enumerate(sorted_features):
                shap_vals = sorted_shap_values[:, i]
                
                scatter = ax_main.scatter(shap_vals, 
                                       [i + np.random.normal(0, 0.08) for _ in range(len(shap_vals))], 
                                       c=shap_vals, cmap='coolwarm', 
                                       s=6, alpha=0.7, edgecolors='none', zorder=5)
                scatter_plot = scatter
                
                # å³ä¾§é‡è¦æ€§ç«–çº¿
                feature_importance = global_importance[feature_names.index(feature)]
                importance_ratio = feature_importance / max_importance
                
                if importance_ratio > 0.66:
                    line_color = '#FF6B6B'
                    line_width = 2.5
                elif importance_ratio > 0.33:
                    line_color = '#FFA726'
                    line_width = 2.0
                else:
                    line_color = '#4CAF50'
                    line_width = 1.5
                
                line_x = shap_abs_max * 1.015
                ax_main.plot([line_x, line_x], [i - 0.35, i + 0.35], 
                           color=line_color, linewidth=line_width, alpha=0.9, zorder=3)

            ax_main.set_xlabel('SHAP Value (Impact on Model Output)', 
                             fontsize=11, fontweight='bold', labelpad=10)
            ax_main.set_ylabel('Features (Sorted by Global Importance)', fontsize=12, fontweight='bold')
            ax_main.axvline(x=0, color='black', linestyle='-', alpha=0.8, linewidth=0.8, zorder=1)
            ax_main.grid(axis='x', alpha=0.2, zorder=0)
            ax_main.tick_params(axis='x', which='major', labelsize=9)

            ax_main.set_yticks(y_pos)
            ax_main.set_yticklabels(sorted_features, fontsize=9)
            ax_main.set_ylim(-0.5, len(sorted_features) - 0.5)

            # è®¾ç½®ä¸»æ ‡é¢˜
            ax_main.set_title('Fusion XGBoost: Dual-Axis SHAP Analysis', 
                           fontsize=14, fontweight='bold', pad=20)

            # æŠŠTop/Bottomè¯´æ˜æ”¾åœ¨æ•´ä¸ªå›¾å½¢çš„é¡¶éƒ¨ - åœ¨tight_layoutä¹‹å‰æ·»åŠ 
            fig.text(0.02, 0.95, 'Top: Global Importance Stacking | Bottom: SHAP Distribution', 
                   fontsize=11, fontweight='normal',
                   verticalalignment='top', horizontalalignment='left')

            # å³ä¾§é¥¼å›¾
            ax_legend.clear()
            category_counts = feature_importance_df['Category'].value_counts()
            colors_pie = [category_colors[cat] for cat in category_counts.index]

            wedges, texts, autotexts = ax_legend.pie(
                category_counts.values,
                labels=[f'{cat.capitalize()}' for cat in category_counts.index],
                colors=colors_pie,
                autopct='%1.1f%%',
                startangle=90,
                explode=[0.03] * len(category_counts),
                shadow=False,
                wedgeprops={'edgecolor': 'white', 'linewidth': 2},
                textprops={'fontsize': 10, 'fontweight': 'bold', 'color': 'white'}
            )

            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')

            ax_legend.set_title('Feature Categories\nDistribution', 
                              fontsize=12, fontweight='bold', pad=20)

            # å›¾ä¾‹
            importance_legend = [
                plt.Line2D([0], [0], color='#FF6B6B', linewidth=2.5, label='High Importance'),
                plt.Line2D([0], [0], color='#FFA726', linewidth=2.0, label='Medium Importance'),
                plt.Line2D([0], [0], color='#4CAF50', linewidth=1.5, label='Low Importance'),
            ]

            ax_legend.legend(handles=importance_legend, loc='lower center', frameon=True, 
                            fontsize=9, bbox_to_anchor=(0.5, -0.15))

            if scatter_plot is not None:
                cax = fig.add_axes([0.78, 0.82, 0.15, 0.02])
                cbar = plt.colorbar(scatter_plot, cax=cax, orientation='horizontal')
                cbar.set_label('Feature Value Impact', fontsize=9, fontweight='bold')

            ax_legend.axis('off')
            
            # åœ¨æ·»åŠ æ‰€æœ‰æ–‡å­—åè°ƒç”¨tight_layout
            plt.tight_layout()

            buf = BytesIO()
            plt.savefig(buf, format="png", dpi=300, bbox_inches='tight')
            plt.close()
            
            data = base64.b64encode(buf.getbuffer()).decode("ascii")
            return f"data:image/png;base64,{data}"
            
    except Exception as e:
        st.error(f"SHAPå¯è§†åŒ–é”™è¯¯: {str(e)}")
        return None

# ==================== å°¿å¸¸è§„åˆ†çº§æŒ‡å— ====================
def show_compact_urine_guide():
    with st.expander(f"ğŸ” {get_text('urine_guide')}", expanded=False):
        if st.session_state.language == 'chinese':
            st.markdown("""
            **åˆ†çº§æ ‡å‡†:**
            - **å°¿ç³–**: 0(-),1(1+),2(2+),3(3+),4(4+)
            - **å°¿è›‹ç™½**: 0(-),1(Â±),2(1+),3(2+),4(3+)
            - **å°¿é…®ä½“**: 0(-),1(Â±),2(1+),3(2+),4(3+)
            - **å°¿æ½œè¡€**: 0(-),1(Â±),2(1+),3(2+),4(3+)
            - **å°¿æ¯”é‡**: 1(<1.010),2(1.010-1.025),3(>1.025),4(å¼‚å¸¸)
            """)
        else:
            st.markdown("""
            **Grading Standards:**
            - **Glucose**: 0(-),1(1+),2(2+),3(3+),4(4+)
            - **Protein**: 0(-),1(Â±),2(1+),3(2+),4(3+)
            - **Ketone**: 0(-),1(Â±),2(1+),3(2+),4(3+)
            - **Occult Blood**: 0(-),1(Â±),2(1+),3(2+),4(3+)
            - **Specific Gravity**: 1(<1.010),2(1.010-1.025),3(>1.025),4(Abnormal)
            """)

# ==================== è¾“å…¥è¡¨å• ====================
st.markdown("---")
st.subheader(get_text("patient_info"))
show_compact_urine_guide()

with st.form("prediction_form"):
    st.write(f"**{get_text('all_indicators')}**")
    
    col1, col2, col3 = st.columns(3)
    
    input_values = {}
    
    with col1:
        input_values['Urine_GlucoseGrade'] = st.selectbox("Urine Glucose Grade" if st.session_state.language == 'english' else "å°¿ç³–ç­‰çº§", [0, 1, 2, 3, 4])
        input_values['Blood_MediumFluorescenceReticulocyte'] = st.number_input("Medium Fluorescence Reticulocyte(%)" if st.session_state.language == 'english' else "ä¸­è§å…‰ç½‘ç»‡çº¢ç»†èƒ(%)", value=1.5, step=0.1)
        input_values['Urine_ProteinGrade'] = st.selectbox("Urine Protein Grade" if st.session_state.language == 'english' else "å°¿è›‹ç™½ç­‰çº§", [0, 1, 2, 3, 4])
        input_values['Blood_LowFluorescenceReticulocyte'] = st.number_input("Low Fluorescence Reticulocyte(%)" if st.session_state.language == 'english' else "ä½è§å…‰ç½‘ç»‡çº¢ç»†èƒ(%)", value=80.0, step=0.1)
        input_values['Urine_Gender'] = st.selectbox("Gender" if st.session_state.language == 'english' else "æ€§åˆ«", [0, 1], format_func=lambda x: "Female" if x == 0 else "Male" if st.session_state.language == 'english' else "å¥³æ€§" if x == 0 else "ç”·æ€§")
    
    with col2:
        input_values['Blood_LymphocyteCount'] = st.number_input("Lymphocyte Count" if st.session_state.language == 'english' else "æ·‹å·´ç»†èƒè®¡æ•°", value=2.0, step=0.1)
        input_values['Blood_RDW_CV'] = st.number_input("RDW-CV" if st.session_state.language == 'english' else "çº¢ç»†èƒåˆ†å¸ƒå®½åº¦CV", value=13.0, step=0.1)
        input_values['Blood_MCHC'] = st.number_input("MCHC" if st.session_state.language == 'english' else "å¹³å‡è¡€çº¢è›‹ç™½æµ“åº¦", value=330.0, step=1.0)
        input_values['Urine_UrineSpecificGravity'] = st.number_input("Urine Specific Gravity" if st.session_state.language == 'english' else "å°¿æ¯”é‡", value=1.015, step=0.001)
        input_values['Blood_LargePlateletRatio'] = st.number_input("Large Platelet Ratio(%)" if st.session_state.language == 'english' else "å¤§è¡€å°æ¿æ¯”ç‡(%)", value=30.0, step=0.1)
    
    with col3:
        input_values['Urine_KetoneGrade'] = st.selectbox("Urine Ketone Grade" if st.session_state.language == 'english' else "å°¿é…®ä½“ç­‰çº§", [0, 1, 2, 3, 4])
        input_values['Blood_PlateletDistributionWidth'] = st.number_input("Platelet Distribution Width" if st.session_state.language == 'english' else "è¡€å°æ¿åˆ†å¸ƒå®½åº¦", value=10.0, step=0.1)
        input_values['Blood_PlateletCount'] = st.number_input("Platelet Count" if st.session_state.language == 'english' else "è¡€å°æ¿è®¡æ•°", value=250.0, step=1.0)
        input_values['Blood_BasophilCount'] = st.number_input("Basophil Count" if st.session_state.language == 'english' else "å—œç¢±æ€§ç²’ç»†èƒè®¡æ•°", value=0.02, step=0.01)
        input_values['Urine_SpecificGravityGrade'] = st.selectbox("Specific Gravity Grade" if st.session_state.language == 'english' else "å°¿æ¯”é‡ç­‰çº§", [0, 1, 2, 3, 4])
    
    col4, col5 = st.columns(2)
    with col4:
        input_values['Urine_Age'] = st.number_input("Age" if st.session_state.language == 'english' else "å¹´é¾„", value=45)
        input_values['Blood_MCH'] = st.number_input("MCH" if st.session_state.language == 'english' else "å¹³å‡è¡€çº¢è›‹ç™½é‡", value=30.0, step=0.1)
    with col5:
        input_values['Urine_OccultBloodGrade'] = st.selectbox("Occult Blood Grade" if st.session_state.language == 'english' else "å°¿æ½œè¡€ç­‰çº§", [0, 1, 2, 3, 4])
        input_values['Blood_EosinophilCount'] = st.number_input("Eosinophil Count" if st.session_state.language == 'english' else "å—œé…¸æ€§ç²’ç»†èƒè®¡æ•°", value=0.1, step=0.01)
    
    submitted = st.form_submit_button(get_text("predict_button"))

# ==================== é¢„æµ‹ç»“æœ ====================
if submitted:
    try:
        features = [[input_values[feature] for feature in feature_names]]
        input_data = pd.DataFrame(features, columns=feature_names)
        
        with st.spinner('æ­£åœ¨è¿›è¡Œé¢„æµ‹...' if st.session_state.language == 'chinese' else 'Predicting...'):
            probability = model.predict_proba(input_data)[0][1] * 100
        
        if probability > 70:
            risk_level = get_text("high_risk")
            suggestion = get_text("high_risk_suggestion")
        elif probability > 30:
            risk_level = get_text("medium_risk")
            suggestion = get_text("medium_risk_suggestion")
        else:
            risk_level = get_text("low_risk")
            suggestion = get_text("low_risk_suggestion")
        
        st.markdown("---")
        st.subheader(get_text("result_title"))
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(get_text("probability"), f"{probability:.1f}%")
        with col2:
            st.metric(get_text("risk_level"), risk_level)
        with col3:
            st.metric(get_text("samples"), "19 indicators")
        
        st.progress(float(probability / 100))
        st.info(f"**{get_text('medical_advice')}**: {suggestion}")
        
        # SHAPå¯è§†åŒ–
        if SHAP_AVAILABLE and explainer is not None:
            st.markdown("---")
            st.subheader(get_text("shap_analysis"))
            
            # åœ¨SHAPå›¾ä¸Šæ–¹æ·»åŠ é—´è·ï¼Œé¿å…è¢«é®æŒ¡
            st.markdown("<div style='margin-bottom: 30px;'></div>", unsafe_allow_html=True)
            
            shap_image = create_clean_shap_plot(input_data, probability)
            if shap_image:
                st.image(shap_image, use_container_width=True)
            
            # ==================== å®Œæ•´å…è´£å£°æ˜ï¼ˆä½¿ç”¨expanderï¼‰ ====================
            st.markdown("---")
            expander_label = f"ğŸ“ {get_text('disclaimer_title')} - ç‚¹å‡»å±•å¼€é˜…è¯»å®Œæ•´å£°æ˜" if st.session_state.language == 'chinese' else f"ğŸ“ {get_text('disclaimer_title')} - Click to expand full disclaimer"
            with st.expander(expander_label, expanded=True):
                st.warning(get_text("disclaimer_content"))
            
    except Exception as e:
        st.error(f"é¢„æµ‹å¤±è´¥: {str(e)}" if st.session_state.language == 'chinese' else f"Prediction failed: {str(e)}")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("System Information" if st.session_state.language == 'english' else "ç³»ç»Ÿä¿¡æ¯")
    st.info("XGBoost-based Diabetes Risk Prediction System" if st.session_state.language == 'english' else "åŸºäºXGBoostçš„ç³–å°¿ç—…é£é™©é¢„æµ‹ç³»ç»Ÿ")